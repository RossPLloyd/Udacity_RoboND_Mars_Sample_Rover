{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project: Search and Sample Return\n",
    "\n",
    "**The goals / steps of this project are the following:**  \n",
    "\n",
    "**Training / Calibration**  \n",
    "\n",
    "* Download the simulator and take data in \"Training Mode\" - DONE\n",
    "* Test out the functions in the Jupyter Notebook provided - DONE / submitted with project\n",
    "* Add functions to detect obstacles and samples of interest (golden rocks) - DONE and successful\n",
    "* Fill in the `process_image()` function with the appropriate image processing steps (perspective transform, color threshold etc.) to get from raw images to a map.  The `output_image` you create in this step should demonstrate that your mapping pipeline works. DONE and successful to greater than 40% coverage and approximately 80% fidelity.\n",
    "* Use `moviepy` to process the images in your saved dataset with the `process_image()` function.  Include the video you produce as part of your submission - DONE as part of notebook stage though screencap used to create submission video.\n",
    "\n",
    "**Autonomous Navigation / Mapping**\n",
    "\n",
    "* Fill in the `perception_step()` function within the `perception.py` script with the appropriate image processing functions to create a map and update `Rover()` data (similar to what you did with `process_image()` in the notebook). - DONE\n",
    "* Fill in the `decision_step()` function within the `decision.py` script with conditional statements that take into consideration the outputs of the `perception_step()` in deciding how to issue throttle, brake and steering commands - DONE \n",
    "* Iterate on your perception and decision function until your rover does a reasonable (need to define metric) job of navigating and mapping - DONE according to baseline submission criteria (40% coverage, greater than 60% fidelity)\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image2]: ../calibration_images/example_grid1.jpg\n",
    "[image3]: ../calibration_images/example_rock2.jpg\n",
    "\n",
    "\n",
    "[Rubric](https://review.udacity.com/#!/rubrics/916/view) Points\n",
    "\n",
    "# Writeup / README\n",
    "Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  \n",
    "---\n",
    "\n",
    "##### Notebook Analysis\n",
    "#### 1. Run the functions provided in the notebook on test images (first with the test data provided, next on data you have recorded). Add/modify functions to allow for color selection of obstacles and rock samples.\n",
    "\n",
    "In the Jupyter notebook I used at first the images provided in the repo, and then later inserted some of my own. Below is an example of the calibration image I used for setting the source transform area and an image of a rock that I took from generated output images from the simulator.\n",
    "\n",
    "![Grid on: Area used for mapping transformed pixels][image2]\n",
    "\n",
    "\n",
    "![Image of one of the sample rocks][image3]\n",
    "\n",
    "Here I experimented with both the methods given in the classes (using the numpy array and setting the threshold limits) and also using openCV \"simple thresholding\" and \"weighted thresholding\" methods. I found that at least on static images the openCV simple threshold method seemed to produce visibly better data, with less overlap between actual and perceived navigable terrain, but in the simulation runs there was little difference. The weighted image was not suitable for \"zeroing out\" navigable vs non-navigable terrain and was rejected. For these reasons I opted to stay with the class methods in the final project. \n",
    "\n",
    "For the provided class methods, the relevant lines of code are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def color_thresh(img, rgb_thresh=(160, 160, 160)):\n",
    "    # Create an array of zeros same xy size as img, but single channel\n",
    "    color_select = np.zeros_like(img[:,:,0])\n",
    "    # Require that each pixel be above all three threshold values in RGB\n",
    "    # above_thresh will now contain a boolean array with \"True\"\n",
    "    # where threshold was met\n",
    "    above_thresh = (img[:,:,0] > rgb_thresh[0]) \\\n",
    "               & (img[:,:,1] > rgb_thresh[1]) \\\n",
    "               & (img[:,:,2] > rgb_thresh[2]) \\\n",
    "            #data.pitch  \n",
    "    # Index the array of zeros with the boolean array and set to 1\n",
    "    color_select[above_thresh] = 1\n",
    "    # Return the binary image\n",
    "    return color_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is for finding navigable terrain, with lighter pixels above the threshold. For obstacles, we take the pixels BELOW the threshold.\n",
    "\n",
    "And for opencv, simple thresholding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-fe2b27869d6f>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-fe2b27869d6f>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    As the threshold function can only use single channel grayscale, separate the source image into 3 channels\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def color_thresh(img):\n",
    "    # Create an array of zeros same xy size as img, but single channel\n",
    "    color_select = np.zeros_like(img[:,:,0])\n",
    "    As the threshold function can only use single channel grayscale, separate the source image into 3 channels\n",
    "    a1 = np.array(img)\n",
    "    a = a1[:,:,0]\n",
    "    b1 = np.array(img)\n",
    "    b = b1[:,:,1]\n",
    "    c1 = np.array(img)\n",
    "    c = c1[:,:,2]\n",
    "\n",
    "    #run the cv2 threshold code on each channel. I used the TRUNC flag as this produced a finer edge to the navigable terrain\n",
    "    ret,trunc_a = cv2.threshold(a,155,255,cv2.THRESH_TRUNC)\n",
    "    ret,trunc_b = cv2.threshold(b,155,255,cv2.THRESH_TRUNC)\n",
    "    ret,trunc_c = cv2.threshold(c,155,255,cv2.THRESH_TRUNC)\n",
    "   \n",
    "    # As before, require that each pixel be above all three threshold values in RGB\n",
    "    # above_thresh will now contain a boolean array with \"True\" where threshold was met\n",
    "    \n",
    "    above_thresh = (trunc_a[:,:] == 155) \\\n",
    "                & (trunc_b[:,:] == 155) \\\n",
    "                & (trunc_c[:,:] == 155)\n",
    "    \n",
    "    # Index the array of zeros with the boolean array and set to 1\n",
    "    color_select[above_thresh] = 1\n",
    "    # Return the binary image\n",
    "    return color_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW: Output images from the TRUNC warp and thresh function, followed by comparison of the two methods, overlaid onto the original, non-thresholded image. The opencv method (middle) gives slightly finer interpetation of the area of \"blown sand\" up the cavern walls than the classroom method (bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]:\n",
    "\n",
    "[image4]: ../writeup_images/ThreshedWarpedCV2TRUNC.jpg\n",
    "[image5]: ../writeup_images/opencvBLEND.jpg\n",
    "[image6]: ../writeup_images/classBLEND.jpg\n",
    "[image7]: ../writeup_images/4.jpg\n",
    "    \n",
    "![Warped / thresholded][image4]\n",
    "\n",
    "![OpenCV][image5]\n",
    "\n",
    "![Classroom method][image6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Populate the `process_image()` function with the appropriate analysis steps to map pixels identifying navigable terrain, obstacles and rock samples into a worldmap.  Run `process_image()` on your test data using the `moviepy` functions provided to create video output of your result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the supporting functions which are then called by the process_image() function, listed underneath. These are mostly \"stock\" from the classroom lessons with some tweaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to perform a perspective transform\n",
    "# I've used the example grid image above to choose source points for the\n",
    "# grid cell in front of the rover (each grid cell is 1 square meter in the sim)\n",
    "\n",
    "def perspect_transform(img, src, dst):           \n",
    "    #define the transformation matrix required by the function\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    #apply the function\n",
    "    warped = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]))# keep same size as input image\n",
    "    #create a mask area so that the region outside the rover's camera is not factored into the final map image\n",
    "    mask = cv2.warpPerspective(np.ones_like(img[:,:,0]), M, (img.shape[1], img.shape[0]))\n",
    "    return warped, mask\n",
    "\n",
    "\n",
    "# Define calibration box in source (actual) and destination (desired) coordinates\n",
    "# These source and destination points are defined to warp the image\n",
    "# to a grid where each 10x10 pixel square represents 1 square meter\n",
    "# The destination box will be 2*dst_size on each side\n",
    "dst_size = 5 \n",
    "# Set a bottom offset to account for the fact that the bottom of the image \n",
    "# is not the position of the rover but a bit in front of it\n",
    "\n",
    "bottom_offset = 6\n",
    "source = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])\n",
    "destination = np.float32([[image.shape[1]/2 - dst_size, image.shape[0] - bottom_offset],\n",
    "                  [image.shape[1]/2 + dst_size, image.shape[0] - bottom_offset],\n",
    "                  [image.shape[1]/2 + dst_size, image.shape[0] - 2*dst_size - bottom_offset], \n",
    "                  [image.shape[1]/2 - dst_size, image.shape[0] - 2*dst_size - bottom_offset],\n",
    "                  ])\n",
    "#finally call the function and provide it with the needed arguments:\n",
    "warped, mask = perspect_transform(grid_img, source, destination)\n",
    "\n",
    "\n",
    "# Define a function to convert from image coordinates to rover coordinates\n",
    "def rover_coords(binary_img):\n",
    "    # Identify nonzero pixels\n",
    "    ypos, xpos = binary_img.nonzero()\n",
    "    # Calculate pixel positions with reference to the rover position being at the \n",
    "    # center bottom of the image.  \n",
    "    x_pixel = -(ypos - binary_img.shape[0]).astype(np.float)\n",
    "    y_pixel = -(xpos - binary_img.shape[1]/2 ).astype(np.float)\n",
    "    return x_pixel, y_pixel\n",
    "\n",
    "# Define a function to convert to radial coords in rover space\n",
    "def to_polar_coords(x_pixel, y_pixel):\n",
    "    # Convert (x_pixel, y_pixel) to (distance, angle) \n",
    "    # in polar coordinates in rover space\n",
    "    # Calculate distance to each pixel\n",
    "    dist = np.sqrt(x_pixel**2 + y_pixel**2)\n",
    "    # Calculate angle away from vertical for each pixel\n",
    "    angles = np.arctan2(y_pixel, x_pixel)\n",
    "    return dist, angles\n",
    "\n",
    "# Define a function to map rover space pixels to world space\n",
    "def rotate_pix(xpix, ypix, yaw):\n",
    "    # Convert yaw to radians\n",
    "    yaw_rad = yaw * np.pi / 180\n",
    "    xpix_rotated = (xpix * np.cos(yaw_rad)) - (ypix * np.sin(yaw_rad))\n",
    "                            \n",
    "    ypix_rotated = (xpix * np.sin(yaw_rad)) + (ypix * np.cos(yaw_rad))\n",
    "    # Return the result  \n",
    "    return xpix_rotated, ypix_rotated\n",
    "\n",
    "def translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale): \n",
    "    # Apply a scaling and a translation\n",
    "    xpix_translated = (xpix_rot / scale) + xpos\n",
    "    ypix_translated = (ypix_rot / scale) + ypos\n",
    "    # Return the result  \n",
    "    return xpix_translated, ypix_translated\n",
    "\n",
    "\n",
    "# Define a function to apply rotation and translation (and clipping)\n",
    "# Once you define the two functions above this function should work\n",
    "def pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale):\n",
    "    # Apply rotation\n",
    "    xpix_rot, ypix_rot = rotate_pix(xpix, ypix, yaw)\n",
    "    # Apply translation\n",
    "    xpix_tran, ypix_tran = translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale)\n",
    "    # Perform rotation, translation and clipping all at once\n",
    "    x_pix_world = np.clip(np.int_(xpix_tran), 0, world_size - 1)\n",
    "    y_pix_world = np.clip(np.int_(ypix_tran), 0, world_size - 1)\n",
    "    # Return the result\n",
    "    return x_pix_world, y_pix_world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an image of the outputs from the above functions. Clockwise from top left: Rover camera image, warped camera image, warped and thresholded image, and the transformed-to-rover coords image plus average steer angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]:\n",
    "    \n",
    "[image7]: ../writeup_images/4.jpg\n",
    "    \n",
    "![ALT TEXT][image7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note that the arrow seen in the last image is generated by the following code:\n",
    "arrow_length = 100\n",
    "x_arrow = arrow_length * np.cos(mean_dir)\n",
    "y_arrow = arrow_length * np.sin(mean_dir)\n",
    "plt.arrow(0, 0, x_arrow, y_arrow, color='red', zorder=2, head_width=10, width=2)\n",
    "\n",
    "#similar code is used later in order to define the steer angle for the rover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now defining the process_image function, which will call the above functions and pass the data to the sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to pass stored images to,\n",
    "# reading rover position and yaw angle from csv file\n",
    "# This function will be used by moviepy to create an output video\n",
    "def process_image(img):\n",
    "    #In the notebook, the instance of the class is called data, however in the functioning rover it is called \"Rover\".\n",
    "   \n",
    "    # The aim of this function is to: \n",
    "    # 1) Define source and destination points for perspective transform and then apply the transform.\n",
    "    # 2) Apply perspective transform\n",
    "    # 3) Apply color threshold to identify navigable terrain/obstacles (rock samples are handled separately below)\n",
    "    # 4) Convert thresholded image pixel values to rover-centric coords\n",
    "    # 5) Convert rover-centric pixel values to world coords\n",
    "    # 6) Update worldmap (to be displayed on right side of screen)\n",
    "        # Example: data.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n",
    "        #          data.worldmap[rock_y_world, rock_x_world, 1] += 1\n",
    "        #          data.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n",
    "        \n",
    "        \n",
    "    # 1) Define source and destination points for perspective transform and then apply the transform. \n",
    "    # Here we are calling the transform function defined above:\n",
    "    warped, mask = perspect_transform(img, source, destination)\n",
    "    #Apply colour theshold to identify navigable terrin / obstacles\n",
    "    threshed = color_thresh(warped)\n",
    "    #create the obstacle map\n",
    "    obs_map = np.absolute(np.float32(threshed) - 1) * mask\n",
    "    xpix, ypix = rover_coords(threshed)\n",
    "    #convert rover-centric pixel values to world co-ordinates\n",
    "    world_size = data.worldmap.shape[0]\n",
    "    #define the size of the target area \n",
    "    scale = 2 * dst_size\n",
    "    #retrieve data from the data bucket\n",
    "    xpos = data.xpos[data.count]\n",
    "    ypos = data.ypos[data.count]\n",
    "    yaw = data.yaw[data.count]\n",
    "    pitch_value = data.pitch[data.count]\n",
    "    \n",
    "    #Go ahead and create the maps to be overlaid:\n",
    "    x_world, y_world = pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale)\n",
    "    obsxpix, obsypix = rover_coords(obs_map) \n",
    "    obs_x_world, obs_y_world = pix_to_world(obsxpix, obsypix, xpos, ypos, yaw, world_size, scale)\n",
    "        \n",
    "    #This section is designed to only update the worldmap when the pitch angle is within certain limits. This is because\n",
    "    #the transform function only produces valid results for a flat plane. In the autononmous driving portion I also applied\n",
    "    #a limit for roll.\n",
    "    \n",
    "    #Navigable terrain\n",
    "    if pitch_value < 0.01:\n",
    "        data.worldmap[y_world, x_world, 2] = 255\n",
    "    elif pitch_value > 355:\n",
    "        data.worldmap[y_world, x_world, 2] = 255\n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "    #Obstacles\n",
    "    if pitch_value < 0.01:\n",
    "        data.worldmap[obs_y_world, obs_x_world, 0] = 255\n",
    "    elif pitch_value > 355:\n",
    "        data.worldmap[obs_y_world, obs_x_world, 0] = 255\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #Lets make our map pixels the right colours:\n",
    "    nav_pix = data.worldmap[:, :, 2] > 0\n",
    "    data.worldmap[nav_pix, 0] = 0\n",
    "          \n",
    "    #Threshold function for finding rocks\n",
    "    rock_map = find_rocks(warped, levels=(110, 110, 50))\n",
    "    if rock_map.any():\n",
    "        rock_x, rock_y = rover_coords(rock_map)\n",
    "        rock_x_world, rock_y_world = pix_to_world(rock_x, rock_y, xpos, ypos, yaw, world_size, scale)\n",
    "        data.worldmap[rock_y_world, rock_x_world, :] = 255\n",
    "    \n",
    "    # 7) Make a mosaic image, this is the stick code from the notebook\n",
    "        # First create a blank image (can be whatever shape you like)\n",
    "    output_image = np.zeros((img.shape[0] + data.worldmap.shape[0], img.shape[1]*2, 3))\n",
    "        # Next you can populate regions of the image with various output\n",
    "        # Here I'm putting the original image in the upper left hand corner\n",
    "    output_image[0:img.shape[0], 0:img.shape[1]] = img\n",
    "\n",
    "        # Let's create more images to add to the mosaic, first a warped image\n",
    "    warped, mask = perspect_transform(img, source, destination)\n",
    "        # Add the warped image in the upper right hand corner\n",
    "    output_image[0:img.shape[0], img.shape[1]:] = warped\n",
    "\n",
    "        # Overlay worldmap with ground truth map\n",
    "    map_add = cv2.addWeighted(data.worldmap, 1, data.ground_truth, 0.5, 0)\n",
    "        # Flip map overlay so y-axis points upward and add to output_image \n",
    "    output_image[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)\n",
    "    \n",
    "    #defining some variables to allow us to, for example, view our pitch and position:\n",
    "    pitch_text = str(data.pitch[data.count])\n",
    "    xpos_text = str(data.xpos[data.count])\n",
    "    ypos_text = str(data.ypos[data.count])\n",
    "\n",
    "        # Then putting some text over the image\n",
    "    cv2.putText(output_image, pitch_text,  (20, 20), \n",
    "                cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n",
    "    cv2.putText(output_image, xpos_text,  (20, 20), \n",
    "                cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n",
    "    \n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autonomous Navigation and Mapping\n",
    "\n",
    "#### 1. Fill in the `perception_step()` (at the bottom of the `perception.py` script) and `decision_step()` (in `decision.py`) functions in the autonomous mapping scripts.\n",
    "\n",
    "The perception step was filled consistent with the explanations provided above, and once the rover_angles were being output correctly, the rover now had some basic ability to self drive.\n",
    "\n",
    "I used the standard \"average angle\" method for steering, which seemed to work very well. I would have liked to have tried a wall following approach, however due to time constraints I went with the average angle.\n",
    "\n",
    "#### Decision Step, and Getting Stuck\n",
    "\n",
    "Resolution: 1920 x 1080\n",
    "Graphics Settings: Fantastic\n",
    "FPS: 45\n",
    "\n",
    "I experimented with trying to implement a \"stuck\" mode for the rover. To do this I tried a number of things, including creating a list inside the rover object that updated perception that kept the absolute value of the product of the global coordinates (See code snippet below). This list, at my sims 45FPS, was capable of containing around 10 seconds of data, and so if the first and last indexes were pretty much the same, then the rover was stuck (see code below that I ultimately removed from the decision step).\n",
    "\n",
    "I also tried comparing speed vs throttle, the logic being that if the throttle was open but the rover not moving, then this would mean it was stuck. This would have helped with the instances where the camera view \"clipped\" through the rocks and led the rover to think that there was clear space in front of it.\n",
    "\n",
    "One problem I ran into was that the rover could sometimes not differentiate between reasons why velocity was zero, such as \"just started the sim\" vs \"I have hit something\". I tried implementing a count so that the stuck routine would not begin until 1000 time steps had elapsed, allowing the rover to begin moving.\n",
    "\n",
    "Another problem I ran into was having other if statements compete with one another, which would lead to \"unsticking\" procedures like backing up or turning being interrupted or just oscillating on the spot.\n",
    "\n",
    "Unfortunately I could not make any of these approaches work consistently, I believe due to some confusion with the if statement topography. I would like to learn more about implementing decision trees in python. However as my rover could fairly regularly complete the required 40% mapping and had good (80%) fidelity, I decided this was sufficient given the time spent on the project.\n",
    "\n",
    "I plan to revisit this project in the future and try some alternative approaches, as well as bolster my knowledge on implementing decision trees in code.\n",
    "\n",
    "AUTONOMOUS DRIVING PERFORMANCE\n",
    "\n",
    "As noted above, I was not able to implement an unsticking routine. The mapping process can therefore fail if the rover drives into a rock as its first action. Clearly an unsticking procedure would help here.\n",
    "\n",
    "In terms of navigation, the rover does a reasonably good job, and it will eventually find the average angle that leads to it mapping the entire area given a stuck-free run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code to implement average angle steering. It takes a mean of the angles to each of the pixels in frame.\n",
    "Rover.steer = np.clip(np.mean(Rover.nav_angles * 180/np.pi), -15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to create buffer of formatted last 10 seconds position data\n",
    "\tRover.xypos_mult = abs((Rover.pos[0] * Rover.pos[1]))\n",
    "\t#this populates the empty list from beginning to end, and then shifts the data to the right once\n",
    "\t#the array size is 450. At 45fps this equates to 10 seconds of data.\n",
    "\tif (len(Rover.buffer_xypos) < 450):\n",
    "\t\tRover.buffer_xypos.insert(0, Rover.xypos_mult)\n",
    "\telif (len(Rover.buffer_xypos) == 450):\n",
    "\t\tRover.buffer_xypos.pop()\n",
    "\t\tRover.buffer_xypos.insert(0, Rover.xypos_mult)\n",
    "\timage = Rover.img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#statement briefly added to the decision step to try and implement the 'stuck' routine. It checks if the rover has not moved\n",
    "# for 10 seconds (data comes from Rover.buffer_xypos), whether the rover can see ahead (obstacles such as rocks are either\n",
    "#transparent due to clipping or some navigable terrain is visible). Finally it checks to make sure the sim has not just\n",
    "#started up. This is because I was struggling to make the logic differentiate between \"I have just started\" and \"I am stuck\"\n",
    "elif Rover.vel == 0 and (abs(Rover.buffer_xypos[-1]) - (Rover.buffer_xypos[0]) < 2) and (len(Rover.nav_angles) >= Rover.stop_forward) and Rover.count > 1000:\n",
    "    Throttle = 0\n",
    "    Rover.steer = 15\n",
    "    Rover.mode = 'Stuck'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
